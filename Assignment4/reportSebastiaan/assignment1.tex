%!TEX root = report.tex

\subsection*{Multiple hidden layers}

\begin{table}[!h]
\centering
\begin{tabular}{ll}
\textbf{Parameter}           & \textbf{Value} \\ \hline  
Learning rate($\eta$) & 0.01   \\
L1 regularization   & 0.00   \\
L2 regularization   & 0.0001 \\
Batch Size          & 20     \\
Max epochs          & 1000   \\
Hidden nodes        & 500   
\end{tabular}
\caption{The parameters used for the Neural Network}
\label{parameters1}
\end{table}

A neural network had to be altered in order to see what the results were when multiple layers were used. The MNIST handwritten digits dataset was used as the data set on which the neural network trained and tested. The parameters used can be seen in Table \ref{parameters1}. Each of the layers added used 500 nodes. The output layer was a logistic regression layer with negative cost likelihood.

\begin{table}[!h]
\centering
\begin{tabular}{llll}
\textbf{\# of hidden layers} & \textbf{Validation error(in \%)} & \textbf{Test error(in \%)} & \textbf{Runtime(in min)} \\ \hline
1       &      1.69   &           1.65    & 35.04              \\
2       &      1.88                     &       1.95    &      35.18       \\
3        &       1.93                    &      2.02    &    1564.34$^{*}$       

\end{tabular}
\caption{The results obtained using different amounts of hidden layers. $^*$This run was done on a laptop, which was shut down occassionally.}
\label{results1}
\end{table}

The results can be seen in Table \ref{results1}. The learning algorithm potentially can run for 1000 epochs, but stops once it finished learning, which meant that it never reached it 1000 epochs. Table \ref{results1} shows that the validation and test error increase when using additional hidden layers. This might be due to overfitting, seeing as the added layers have the same amount of input and outputs of the layer before them. This believe is further evidenced since the error increase for the amount of hidden layer used.

\subsection*{Linear Output layers with Mean Squared Error}

The logistic regression output layer with negative cost likelihood is replaced by a linear output layer with a mean squared error cost function. It runs for about 20 epochs, where the mean squared error goes from 120 to 76.