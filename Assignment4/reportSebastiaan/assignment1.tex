%!TEX root = report.tex

\subsection*{Multiple hidden layers}

\begin{table}[!h]
\centering
\label{parameters1}
\begin{tabular}{ll}
\textbf{Parameter}           & \textbf{Value} \\ \hline  
Learning rate($\eta$) & 0.01   \\
L1 regularization   & 0.00   \\
L2 regularization   & 0.0001 \\
Batch Size          & 20     \\
Max epochs          & 1000   \\
Hidden nodes        & 500   
\end{tabular}
\caption{The parameters used for the Neural Network}
\end{table}

A neural network had to be altered in order to see what the results were when multiple layers were used. The MNIST handwritten digits dataset was used as the data set on which the neural network trained and tested. The parameters used can be seen in Table \ref{parameters1}. Each of the layers added used 500 nodes. The output layer was a logistic regression layer with negative cost likelihood.

\begin{table}[!h]
\centering
\label{results}
\begin{tabular}{llll}
\textbf{\# of hidden layers} & \textbf{Validation error} & \textbf{Test error} & \textbf{\# of epochs} \\ \hline
1                            &                           &                     &                       \\
2                            &                           &                     &                       \\
3                            &                           &                     &                      
\end{tabular}
\caption{The results obtained using different amounts of hidden layers.}
\end{table}

The results can be seen in Table \ref{results}. The learning algorithm potentially can run for 1000 epochs, but stops once it finished learning.

\subsection*{Linear Output layers with Mean Squared Error}

The logistic regression output layer with negative cost likelihood is replaced by a linear output layer with a mean squared error cost function. It runs for about 20 epochs, where the mean squared error goes from 120 to 76.